{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB RNN Using Tensor Flow and Keras\n",
    "Sentiment analysis on the IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use bash install to ensure environment setup\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. This code will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "# code to optimize GPU usage\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use the first GPU\n",
    "\n",
    "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU was detected. This code will run on CPU.\")\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nessicary imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Prepare the IMDB dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_data = imdb.load_data()\n",
    "#print(movie_data)\n",
    "\n",
    "# set the number of words to consider to the n most frequent words\n",
    "max_features = 10000\n",
    "\n",
    "#set the sequence length, padds shorters sequences and truncates shorter ones\n",
    "maxlen = 500\n",
    "\n",
    "# load the IMDB data into \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# imloy reversing the data set for better sentament analysis and pad the sequences\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the RNN Architecture using Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          49408     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,362,497\n",
      "Trainable params: 1,362,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the input layer \n",
    "inputs = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# add an  embedded layer that encodes the input vector of size 128\n",
    "x = tf.keras.layers.Embedding(max_features, 128)(inputs)\n",
    "\n",
    "# add long short term memory with 64 units and return entire sequence\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "\n",
    "#add another layer with 64 units and do not return the full sequence just output\n",
    "x = LSTM(64)(x)\n",
    "\n",
    "# define the structure of the output as dense layer with single neuron\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# define model with inputs and outputs\n",
    "model = Model(inputs,outputs)\n",
    "\n",
    "# print a model summary to check design\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer set to 'adam' to adaptively adjust weights\n",
    "# choose loss and metrics to match binary output\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 192s 1s/step - loss: 0.5779 - accuracy: 0.7057 - val_loss: 0.5047 - val_accuracy: 0.7568\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 183s 1s/step - loss: 0.3298 - accuracy: 0.8693 - val_loss: 0.3907 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 192s 1s/step - loss: 0.3060 - accuracy: 0.8794 - val_loss: 0.3702 - val_accuracy: 0.8448\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.1982 - accuracy: 0.9294 - val_loss: 0.3664 - val_accuracy: 0.8560\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 181s 1s/step - loss: 0.1486 - accuracy: 0.9503 - val_loss: 0.3580 - val_accuracy: 0.8646\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.1147 - accuracy: 0.9644 - val_loss: 0.4483 - val_accuracy: 0.8642\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 172s 1s/step - loss: 0.0855 - accuracy: 0.9743 - val_loss: 0.5241 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 189s 1s/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.5490 - val_accuracy: 0.8560\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.0966 - accuracy: 0.9677 - val_loss: 0.4942 - val_accuracy: 0.8488\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.0927 - accuracy: 0.9694 - val_loss: 0.4807 - val_accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "# trains for 10 epochs\n",
    "# update weights every 128 samples\n",
    "# keep 20% of the data for validation\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 113s 144ms/step - loss: 0.5500 - accuracy: 0.8198\n",
      "Test Loss: 0.5500058531761169, Test Accuracy: 0.8198400139808655\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test data set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# print the loss\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generates output predictions for the input samples from the test set.\n",
    "predictions = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"imdb_rnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0091227 ]\n",
      " [0.99352515]\n",
      " [0.9960085 ]\n",
      " ...\n",
      " [0.01743037]\n",
      " [0.29827625]\n",
      " [0.29701084]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
