{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB RNN Using Tensor Flow and Keras\n",
    "Sentiment analysis on the IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.5.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/npacey/.local/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/npacey/.local/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.38.4)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf>=3.9.2->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow) (1.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/npacey/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/npacey/.local/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/npacey/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/npacey/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/npacey/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "# use bash install to ensure environment setup\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. This code will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "# code to optimize GPU usage\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use the first GPU\n",
    "\n",
    "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU was detected. This code will run on CPU.\")\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nessicary imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Prepare the IMDB dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_data = imdb.load_data()\n",
    "#print(movie_data)\n",
    "\n",
    "# set the number of words to consider to the n most frequent words\n",
    "max_features = 10000\n",
    "\n",
    "#set the sequence length, padds shorters sequences and truncates shorter ones\n",
    "maxlen = 500\n",
    "\n",
    "# load the IMDB data into \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# imloy reversing the data set for better sentament analysis and pad the sequences\n",
    "x_train = [x[::-1] for x in x_train]\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "x_test = [x[::-1] for x in x_test]\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the RNN Architecture using Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, None, 64)          49408     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,362,497\n",
      "Trainable params: 1,362,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the input layer \n",
    "inputs = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# add an  embedded layer that encodes the input vector of size 128\n",
    "x = tf.keras.layers.Embedding(max_features, 128)(inputs)\n",
    "\n",
    "# add long short term memory with 64 units and return entire sequence\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "\n",
    "#add another layer with 64 units and do not return the full sequence just output\n",
    "x = LSTM(64)(x)\n",
    "\n",
    "# define the structure of the output as dense layer with single neuron\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# define model with inputs and outputs\n",
    "model = Model(inputs,outputs)\n",
    "\n",
    "# print a model summary to check design\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer set to 'adam' to adaptively adjust weights\n",
    "# choose loss and metrics to match binary output\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 192s 1s/step - loss: 0.5779 - accuracy: 0.7057 - val_loss: 0.5047 - val_accuracy: 0.7568\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 183s 1s/step - loss: 0.3298 - accuracy: 0.8693 - val_loss: 0.3907 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 192s 1s/step - loss: 0.3060 - accuracy: 0.8794 - val_loss: 0.3702 - val_accuracy: 0.8448\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.1982 - accuracy: 0.9294 - val_loss: 0.3664 - val_accuracy: 0.8560\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 181s 1s/step - loss: 0.1486 - accuracy: 0.9503 - val_loss: 0.3580 - val_accuracy: 0.8646\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.1147 - accuracy: 0.9644 - val_loss: 0.4483 - val_accuracy: 0.8642\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 172s 1s/step - loss: 0.0855 - accuracy: 0.9743 - val_loss: 0.5241 - val_accuracy: 0.8372\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 189s 1s/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.5490 - val_accuracy: 0.8560\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.0966 - accuracy: 0.9677 - val_loss: 0.4942 - val_accuracy: 0.8488\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 193s 1s/step - loss: 0.0927 - accuracy: 0.9694 - val_loss: 0.4807 - val_accuracy: 0.8430\n"
     ]
    }
   ],
   "source": [
    "# trains for 10 epochs\n",
    "# update weights every 128 samples\n",
    "# keep 20% of the data for validation\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 113s 144ms/step - loss: 0.5500 - accuracy: 0.8198\n",
      "Test Loss: 0.5500058531761169, Test Accuracy: 0.8198400139808655\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test data set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# print the loss\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Generates output predictions for the input samples from the test set.\n",
    "predictions = model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"imdb_rnn_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0091227 ]\n",
      " [0.99352515]\n",
      " [0.9960085 ]\n",
      " ...\n",
      " [0.01743037]\n",
      " [0.29827625]\n",
      " [0.29701084]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
